{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiLSTM-CRF NER Exploration\n",
    "\n",
    "This notebook explores the biomedical NER dataset and trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "from src.utils.vocab import Vocabulary, LabelVocabulary\n",
    "from src.data.dataset import get_data_statistics\n",
    "from src.models.bilstm_crf import BiLSTMCRF\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistics for all splits\n",
    "train_stats = get_data_statistics('../data/processed/train.txt')\n",
    "dev_stats = get_data_statistics('../data/processed/dev.txt')\n",
    "test_stats = get_data_statistics('../data/processed/test.txt')\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Train: {train_stats['num_sentences']} sentences\")\n",
    "print(f\"Dev:   {dev_stats['num_sentences']} sentences\")\n",
    "print(f\"Test:  {test_stats['num_sentences']} sentences\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentence length distribution\n",
    "def read_sentence_lengths(filepath):\n",
    "    lengths = []\n",
    "    current_length = 0\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if current_length > 0:\n",
    "                    lengths.append(current_length)\n",
    "                    current_length = 0\n",
    "            else:\n",
    "                current_length += 1\n",
    "        if current_length > 0:\n",
    "            lengths.append(current_length)\n",
    "    \n",
    "    return lengths\n",
    "\n",
    "train_lengths = read_sentence_lengths('../data/processed/train.txt')\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.hist(train_lengths, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Sentence Length (tokens)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sentence Lengths in Training Set')\n",
    "plt.axvline(np.mean(train_lengths), color='red', linestyle='--', label=f'Mean: {np.mean(train_lengths):.1f}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution\n",
    "label_dist = train_stats['label_distribution']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "labels = list(label_dist.keys())\n",
    "counts = list(label_dist.values())\n",
    "colors = ['green' if l == 'O' else 'blue' if 'Chemical' in l else 'red' for l in labels]\n",
    "\n",
    "plt.bar(labels, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Label Distribution in Training Set')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print percentages\n",
    "total = sum(counts)\n",
    "print(\"\\nLabel Percentages:\")\n",
    "for label, count in label_dist.items():\n",
    "    percentage = (count / total) * 100\n",
    "    print(f\"{label:15s}: {count:6d} ({percentage:5.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sample_sentences(filepath, n=5):\n",
    "    sentences = []\n",
    "    current_tokens = []\n",
    "    current_tags = []\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if current_tokens:\n",
    "                    sentences.append((current_tokens, current_tags))\n",
    "                    current_tokens = []\n",
    "                    current_tags = []\n",
    "                if len(sentences) >= n:\n",
    "                    break\n",
    "            else:\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) == 2:\n",
    "                    current_tokens.append(parts[0])\n",
    "                    current_tags.append(parts[1])\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Display sample sentences with highlighting\n",
    "samples = read_sample_sentences('../data/processed/train.txt', n=5)\n",
    "\n",
    "print(\"Sample Annotated Sentences:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, (tokens, tags) in enumerate(samples, 1):\n",
    "    print(f\"\\nSentence {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Print tokens with tags\n",
    "    for token, tag in zip(tokens, tags):\n",
    "        if tag.startswith('B-'):\n",
    "            entity_type = tag[2:]\n",
    "            print(f\"[{token}\", end='')\n",
    "        elif tag.startswith('I-'):\n",
    "            print(f\" {token}\", end='')\n",
    "        else:  # O tag\n",
    "            print(f\" {token}\", end='')\n",
    "        \n",
    "        # Close entity if needed\n",
    "        if tag.startswith('B-') or tag.startswith('I-'):\n",
    "            # Check if next tag is not continuation\n",
    "            idx = list(zip(tokens, tags)).index((token, tag))\n",
    "            if idx == len(tags) - 1 or not tags[idx + 1].startswith('I-'):\n",
    "                entity_type = tag[2:]\n",
    "                print(f\"/{entity_type}]\", end='')\n",
    "    \n",
    "    print()  # New line after sentence\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vocabulary Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabularies (if they exist)\n",
    "try:\n",
    "    word_vocab = Vocabulary.load('../artifacts/vocab_word.pkl')\n",
    "    label_vocab = LabelVocabulary.load('../artifacts/vocab_label.pkl')\n",
    "    \n",
    "    print(f\"Word vocabulary size: {len(word_vocab)}\")\n",
    "    print(f\"Label vocabulary: {list(label_vocab.label2idx.keys())}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Vocabularies not found. Run training first to generate vocabularies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Analysis (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model (if it exists)\n",
    "try:\n",
    "    checkpoint = torch.load('../artifacts/best_model.pt', map_location='cpu')\n",
    "    \n",
    "    print(\"Model Checkpoint Information:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Best epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"Best F1 score: {checkpoint.get('best_f1', 'N/A'):.4f}\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Model checkpoint not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize CRF transition matrix (if model is loaded)\n",
    "try:\n",
    "    # Get transition matrix from checkpoint\n",
    "    transitions = checkpoint['model_state_dict']['crf.transitions'].cpu().numpy()\n",
    "    \n",
    "    # Get label names\n",
    "    label_names = list(label_vocab.idx2label.values())\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(transitions, \n",
    "                xticklabels=label_names, \n",
    "                yticklabels=label_names,\n",
    "                cmap='RdYlGn',\n",
    "                center=0,\n",
    "                annot=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={'label': 'Transition Score'})\n",
    "    plt.xlabel('To Tag')\n",
    "    plt.ylabel('From Tag')\n",
    "    plt.title('CRF Transition Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"- Green (positive): Likely transition\")\n",
    "    print(\"- Red (negative): Unlikely transition\")\n",
    "    print(\"- Notice that I-X tags typically follow B-X or I-X tags of the same type\")\n",
    "    \n",
    "except (NameError, KeyError, FileNotFoundError):\n",
    "    print(\"Cannot visualize transitions. Train model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prediction Examples (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make predictions on custom sentences\n",
    "# This requires the full model to be loaded - implementation left as exercise\n",
    "\n",
    "print(\"To make predictions on custom text:\")\n",
    "print(\"1. Load the trained model\")\n",
    "print(\"2. Tokenize your input\")\n",
    "print(\"3. Convert tokens to IDs using word_vocab\")\n",
    "print(\"4. Run model.predict()\")\n",
    "print(\"5. Convert predicted IDs to tags using label_vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis (After Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read predictions file if it exists\n",
    "try:\n",
    "    predictions_df = pd.read_csv('../reports/predictions.txt', \n",
    "                                  sep='\\t', \n",
    "                                  skip_blank_lines=False,\n",
    "                                  names=['TOKEN', 'TRUE_TAG', 'PRED_TAG', 'CORRECT'])\n",
    "    \n",
    "    # Filter out header and separator rows\n",
    "    predictions_df = predictions_df[predictions_df['TOKEN'] != 'TOKEN']\n",
    "    predictions_df = predictions_df[~predictions_df['TOKEN'].str.startswith('=')]\n",
    "    \n",
    "    # Analyze errors\n",
    "    errors = predictions_df[predictions_df['CORRECT'] == 'âœ—']\n",
    "    \n",
    "    print(f\"Total predictions: {len(predictions_df)}\")\n",
    "    print(f\"Errors: {len(errors)}\")\n",
    "    print(f\"Accuracy: {(len(predictions_df) - len(errors)) / len(predictions_df) * 100:.2f}%\")\n",
    "    \n",
    "    # Show some error examples\n",
    "    print(\"\\nSample Errors:\")\n",
    "    print(errors.head(20))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Predictions file not found. Run evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. Dataset statistics and visualizations\n",
    "2. Sample annotated sentences\n",
    "3. Vocabulary analysis\n",
    "4. Model checkpoint inspection\n",
    "5. CRF transition matrix visualization\n",
    "6. Error analysis tools\n",
    "\n",
    "Use this as a starting point for deeper analysis of your NER system!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
